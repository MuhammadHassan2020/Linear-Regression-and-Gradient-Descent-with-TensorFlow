# Linear Regression and Gradient Descent with TensorFlow

This repository contains Python scripts and Jupyter Notebooks that demonstrate **linear regression** using **TensorFlow**, **NumPy**, and **Matplotlib**, as well as the application of different optimization algorithms for training models.

---

## Table of Contents

- [Introduction](#introduction)
- [Features](#features)
- [Getting Started](#getting-started)
- [Installation](#installation)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [Optimization Techniques](#optimization-techniques)
- [Gradient Calculation](#gradient-calculation)
- [Visualization](#visualization)
- [Future Improvements](#future-improvements)
- [Contributing](#contributing)
- [License](#license)

---

## Introduction

This project explores **linear regression** by optimizing the weights of a model using **gradient descent** and various optimization algorithms such as:

- **Stochastic Gradient Descent (SGD)**
- **Momentum-based SGD**
- **Adam Optimizer**
- **AdamW (Adaptive Momentum with Weight Decay)**
- **RMSprop Optimizer**

Additionally, it demonstrates how to calculate gradients using TensorFlow's `GradientTape` and visualize the results.

---

## Features

- Training linear regression models using different optimization algorithms.
- Gradient calculation using TensorFlow for both single-variable and multi-variable functions.
- Visualization of loss convergence during training.
- 3D visualization of gradient descent paths.
- Comparison of optimizer performance.

---

## Getting Started

Follow these instructions to get a copy of the project and run it on your local machine.

---

## Installation

Ensure you have the following dependencies installed:

```bash
pip install numpy tensorflow matplotlib
